This project implements a BERT-based Seq2Seq model for text summarization. It uses pretrained BERT embeddings in a custom encoder-decoder architecture. Input articles and summaries are tokenized, padded, and fed to the model for training. The generated summaries are evaluated using ROUGE metrics to assess n-gram overlap with reference summaries. The project includes training, inference, and evaluation pipelines.
